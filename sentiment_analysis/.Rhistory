demo()
first_ten <- c(10 : 20)
print(first_ten)
?seq
next_ten <- c(from = 21, to = 30)
print(next_ten)
print(next_ten)
next_ten <- c(seq(from = 21, to = 30))
print(next_ten)
all_numbers <- (first_ten + next_ten)
all_numbers <- (first_ten, next_ten)
all_numbers <- c(first_ten, next_ten)
print(all_numbers)
eleventh <- (all_numbers[11])
print(eleventh)
some_numbers <- c(seq(from = all_numbers[2], to = all_numbers[5]))
print(some_numbers)
some_numbers <- c(seq(from = all_numbers[2], to = all_numbers[6]))
print(some_numbers)
even <- c(seq(from = 1, to = 50, by = from + 1 ))
print(even)
even <- c(seq(from = 1, to = 50, by = 1 + 1 ))
print(even)
even <- c(seq(from = 1, to = 50, by =  +1 ))
print(even)
even <- c(seq(from = 1, to = 50, by =  - 1 ))
print(even)
even <- c(seq(from = 1, to = 100, by = 1 + 1 ))
print(even)
even <- c(seq(from = 2, to = 100, by = 1 + 1 ))
print(even)
# Using the `all()` function and `%%` (modulo) operator, confirm that all of the
# numbers in your `even` vector are even
?all
?%%
all()
?%%
all()
all(even %% 2)
all(even %% 1)
prefix <- c(seq(from = phone_numbers[1], to = phone_numbers[4])
print(prefix)
prefix <- c(seq(from = phone_numbers[1], to = phone_numbers[4])
print(prefix)
phone_numbers <- c(8, 6, 7, 5, 3, 0, 9)
prefix <- c(seq(from = phone_numbers[1], to = phone_numbers[4])
prefix <- c(seq(from = phone_numbers[1], to = phone_numbers[4])
print(prefix)
prefix <- c(seq(from = phone_numbers[1], to = phone_numbers[4])
print(prefix)
phone_numbers <- c(8, 6, 7, 5, 3, 0, 9)
prefix <- c(seq(from = phone_numbers[1], to = phone_numbers[4])
rg
prefix <- seq(from = phone_numbers[1], to = phone_numbers[4])
print(prefix)
small <- c(phone_numbers <= 5)
small <- c(phone_numbers[phone_numbers < 6]])
small <- c(phone_numbers[phone_numbers < 6])
large <- c(phone_numbers[phone_numbers > 5])
replace(phone_numbers, phone_numbers > 5, 5)
replace(phone_numbers, phone_numbers %% 1, 0)
replace(phone_numbers, phone_numbers > 5, 5)
replace(phone_numbers, phone_numbers %% 1, 0)
replace(phone_numbers, phone_numbers %% 0, 0)
replace(phone_numbers, phone_numbers > 5, 5)
replace(phone_numbers, phone_numbers %% 0, 0)
replace(phone_numbers, phone_numbers %% 2, 0)
replace(phone_numbers, phone_numbers > 5, 5)
replace(phone_numbers, phone_numbers %% 0, 0)
replace(phone_numbers, 1 = phone_numbers %% 2, 0)
replace(phone_numbers, 1 == phone_numbers %% 2, 0)
all(0 == even %% 2)
numbers <- c(1:201)
squared_numbers <- c(numbers * numbers)
squared_mean <- mean(numbers)
?mean
example_numbers <- c(1:5)
example_mean <- mean(example_numbers)
numbers <- c(1:201)
squared_numbers <- c(numbers * numbers)
squared_mean <- mean(squared_numbers)
square_median <- median(squared_numbers)
?round
squares <- numbers[sqrt(numbers) == round(numbers)]
squares <- numbers
squares <- sqrt(numbers)
new_squares <- c(squares == round(numbers, 0))
squares <- c(sqrt(numbers) == round(numbers, 0))
squared_root <- sqrt(numbers)
squares <- c(squared_root == round(numbers, 0))
squares <- c(squared_root == round(numbers, 0))
squared_root <- sqrt(numbers)
squares <- numbers[sqrt(numbers) == round(numbers)]
numbers <- c(1:201)
squared_numbers <- c(numbers * numbers)
squared_mean <- mean(squared_numbers)
square_median <- median(squared_numbers)
squared_root <- sqrt(numbers)
squares <- c(squared_root)
squares <- c(squared_root == numbers)
squares <- c(squared_root == round(numbers))
squares <- sqrt(numbers)
new_squares <- c(squares == round(numbers,0))
new_squares <- c(squares == round(numbers,0))
new_squares <- c(squares == round(numbers,0))
new_squares <- c(squares == round(numbers,0))
new_squares
new_squares <- c(squares == round(squares,0))
squares[filter]
squares <- numbers
numbers <- c(1:201)
#################
#### PART 1 #####
#################
library(stringr)
my_age <- 20
my_name <- "Israel"
make_introduction <- function(name, age) {
introduction <- paste("Hello, my name is", name, "and I'm", age, "years old.")
}
my_intro <- make_introduction(my_name, my_age)
casual_intro <- sub("Hello, my name is","Hey, I'm", my_intro)
capital_intro <- str_to_title(my_intro)
intro_e_count <- str_count(my_intro, "e")
#################
#### PART 2 #####
#################
books <- c("The Line Becomes a River", "Sapiens",
"Scott Pilgrim & the Infinite Sadness", "To Kill a Mockingbird",
"The Illiad", "The Old Man and the Sea")
top_three_books <- c(books[1:3])
book_reviews <- paste(books, "is a great read!")
remove_book <- function(vector_of_books, index_number) {
book <- vector_of_books[-index_number]
}
books_without_four <- remove_book(books, 4)
long_titles <- books[str_length(books) > 15]
#################
#### PART 3 #####
#################
numbers <- c(1:201)
squared_numbers <- c(numbers * numbers)
squared_mean <- mean(squared_numbers)
square_median <- median(squared_numbers)
squares <- numbers
?round
#################
#### PART 4 #####
#################
# Your script for Part 1 goes here (and delete this comment!)
squares <- numbers[round(sqrt(numbers))]
squares <- numbers[sqrt(numbers)]
squares <- numbers[sqrt(numbers) == round(numbers)]
squares_example <- sqrt(numbers) == round(numbers)
squares_example <- sqrt(numbers) == round()
squares_example <- sqrt(numbers) == round(0)
squares_example <- sqrt(numbers) == round(x ,0)
squares_example <- sqrt(numbers) == round(squared_numbers)
squares <- numbers[sqrt(numbers) == round(sqrt(numbers))]
squares <- numbers[sqrt(numbers) == round(sqrt(numbers))]
square_rooted_numbers <- sqrt(numbers)
squares <- numbers[square_rooted_numbers == round(square_rooted_numbers)]
?as.Date
summer_break <- as.Date(dates, "06/09/18")
summer_break <- as.Date("06/09/18")
summer_break <- as.Date("2018/06/09")
summer_break <- as.Date("2018/31/09")
summer_break <- as.Date("2018/06/31")
summer_break <- as.Date("2018/06/29")
summer_break <- as.Date("2018/06/09")
today <- Sys.Date()
# Draw a plot of the `diamonds_sample` data (price by carat), with both points
# for each diamond AND smoothed lines for each cut (hint: in a separate color)
# Give the points an `alpha` (transparency) of 0.3 to make the plot look nicer
ggplot(data = diamonds_sample) +
geom_point(mapping = aes(x = carat, y = price, color = cut), alpha = 0.3) +
geom_smooth(mapping = aes(x = carat, y = price, color = cut), se = FALSE) +
facet_wrap(~cut)
library(ggplot2)
library(dplyr)
View(mpg)
# Draw a plot of the `diamonds_sample` data (price by carat), with both points
# for each diamond AND smoothed lines for each cut (hint: in a separate color)
# Give the points an `alpha` (transparency) of 0.3 to make the plot look nicer
ggplot(data = diamonds_sample) +
geom_point(mapping = aes(x = carat, y = price, color = cut), alpha = 0.3) +
geom_smooth(mapping = aes(x = carat, y = price, color = cut), se = FALSE) +
facet_wrap(~cut)
diamonds_sample <- sample_n(diamonds, 1000)
# Draw a plot of the `diamonds_sample` data (price by carat), with both points
# for each diamond AND smoothed lines for each cut (hint: in a separate color)
# Give the points an `alpha` (transparency) of 0.3 to make the plot look nicer
ggplot(data = diamonds_sample) +
geom_point(mapping = aes(x = carat, y = price, color = cut), alpha = 0.3) +
geom_smooth(mapping = aes(x = carat, y = price, color = cut), se = FALSE) +
facet_wrap(~cut)
?geom_smooth()
# Draw a plot of the `diamonds_sample` data (price by carat), with both points
# for each diamond AND smoothed lines for each cut (hint: in a separate color)
# Give the points an `alpha` (transparency) of 0.3 to make the plot look nicer
ggplot(data = diamonds_sample) +
geom_point(mapping = aes(x = carat, y = price, color = cut), alpha = 0.3) +
geom_smooth(mapping = aes(x = carat, y = price, color = cut), se = TRUE) +
facet_wrap(~cut)
# Draw a plot of the `diamonds_sample` data (price by carat), with both points
# for each diamond AND smoothed lines for each cut (hint: in a separate color)
# Give the points an `alpha` (transparency) of 0.3 to make the plot look nicer
ggplot(data = diamonds_sample) +
geom_point(mapping = aes(x = carat, y = price, color = cut), alpha = 0.3) +
geom_smooth(mapping = aes(x = carat, y = price, color = cut), se = FALSE) +
facet_wrap(~cut)
## Uncomment this to install packages
install.packages('rvest')
# Load in 'rvest' package
library('rvest')
url <- 'http://www.imdb.com/search/title?count=100&release_date=2016,2016&title_type=feature'
webpage <- read_html(url)
rank_data_html <- html_nodes(webpage,'.text-primary')
rank_data <- html_text(rank_data_html)
head(rank_data)
rank_data<-as.numeric(rank_data)
head(rank_data)
install.packages('syuzhet')
library(syuzhet)
# Create a vector of emotional sentences.
# Add some happy ones, angry ones - you name it!
student_sentences <- c('I really like the pie you gave me this morning.',
'Your shoes suck and are just plain ugly.',
'I\'d really truly love going out in this weather!'
)
# Analyze sentiment for student sentences
# Analyze sentiment for student sentences
student_sentiments <- get_sentiment(student_sentences, method = 'syuzhwt')
# Analyze sentiment for student sentences
student_sentiments <- data.frome(get_sentiment(student_sentences, method = 'syuzhet'))
# Analyze sentiment for student sentences
student_sentiments <- data.frame(get_sentiment(student_sentences, method = 'syuzhet'))
# Analyze sentiment for student sentences
student_sentiments <- data.frame(get_sentiment(student_sentences, method = 'syuzhet'))
student_analysis <- cbind(sentence = student_sentences, student_sentiments)
View(student_analysis)
install.packages('dplyr')
install.packages('tidytext')
install.packages('tidyr')
library(dplyr)
library(stringr)
library(tidytext)
library(tidyr)
library(ggplot2)
library(dplyr)
library(stringr)
library(tidytext)
library(tidyr)
library(ggplot2)
library(dplyr)
library(stringr)
library(tidytext)
library(tidyr)
library(ggplot2)
library(dplyr)
library(stringr)
library(tidytext)
library(tidyr)
library(ggplot2)
library(stringr)
library(tidytext)
library(tidyr)
library(ggplot2)
View(get_sentiments("bing"))
bing_sentiments <- (get_sentiments("bing"))
books <- read.csv('./data/austen_books.csv', stringsAsFactors = FALSE)
setwd("~/Desktop/mini-demos/sentiment_analysis")
books <- read.csv('./data/austen_books.csv', stringsAsFactors = FALSE)
# Map each word in the 'books' dataset to its dictionary-prescribed sentiment.
jane_austen_sentiment <- books %>%
inner_join(bing_sentiments, by = "word")
View(jane_austen_sentiment)
# Instead of having each individual word, count the number of positive/negative
# words in each chapter.
jane_austen_sentiment <- jane_austen_sentiment %>%
count(book, chapter, sentiment)
View(jane_austen_sentiment)
# A chapter's overarching feeling will be calculated by the number of positive
# words minus the number of negative words. Create a new column called
# 'sentiment' with this value.
jane_austen_sentiment <- jane_austen_sentiment %>%
spread(sentiment, n, fill = 0)
View(jane_austen_sentiment)
# Map each word in the 'books' dataset to its dictionary-prescribed sentiment.
jane_austen_sentiment <- books %>%
inner_join(bing_sentiments, by = "word")
# A chapter's overarching feeling will be calculated by the number of positive
# words minus the number of negative words. Create a new column called
# 'sentiment' with this value.
jane_austen_sentiment <- jane_austen_sentiment %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
bing_sentiments <- (get_sentiments("bing"))
books <- read.csv('./data/austen_books.csv', stringsAsFactors = FALSE)
# Map each word in the 'books' dataset to its dictionary-prescribed sentiment.
jane_austen_sentiment <- books %>%
inner_join(bing_sentiments, by = "word")
# A chapter's overarching feeling will be calculated by the number of positive
# words minus the number of negative words. Create a new column called
# 'sentiment' with this value.
jane_austen_sentiment <- jane_austen_sentiment %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
# Instead of having each individual word, count the number of positive/negative
# words in each chapter.
jane_austen_sentiment <- jane_austen_sentiment %>%
count(book, chapter, sentiment)
# A chapter's overarching feeling will be calculated by the number of positive
# words minus the number of negative words. Create a new column called
# 'sentiment' with this value.
jane_austen_sentiment <- jane_austen_sentiment %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
ggplot(jane_austen_sentiment, aes(chapter, sentiment, fill = book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book,ncol = 2, scales = FALSE)
ggplot(jane_austen_sentiment, aes(chapter, sentiment, fill = book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book,ncol = 2)
#load library
library(class) #Has the knn function
#loading data
data("iris")
#Set the seed for reproducibility
set.seed(4948493)
ir_sample <- sample(1:nrow(iris),size=nrow(iris)*.7)
ir_train <- iris[ir_sample,] #Select the 70% of rows
ir_test <- iris[-ir_sample,] #Select the 30% of rows
#Set the seed for reproducibility
set.seed(4948493)
#Sample the Iris data set (70% train, 30% test)
ir_sample <- sample(1:nrow(iris),size=nrow(iris)*.7)
ir_train <- iris[ir_sample,] #Select the 70% of rows
ir_test <- iris[-ir_sample,] #Select the 30% of rows
#Find Accuracy of Prediction
accuracy = function(actual, predicted) {
mean(actual == predicted)
}
#test for single k
pred <- knn(train = scale(ir_train[,-5]),
test = scale(ir_test[,-5]),
cl = ir_train$Species,
k = 5)
pred
accuracy
accuracy(ir_test$Species, pred)
k_to_try = 1:100
acc_k = rep(x = 0, times = length(k_to_try))
for(i in seq_along(k_to_try)) {
pred <- knn(train = scale(ir_train[,-5]),
test = scale(ir_test[,-5]),
cl = ir_train$Species,
k = k_to_try[i])
acc_k[i] <- accuracy(ir_test$Species, pred)
}
plot(acc_k, type = "b", col = "dodgerblue", cex = 1, pch = 20,
xlab = "k, number of neighbors", ylab = "classification accuracy",
main = "Accuracy vs Neighbors")
# add lines indicating k with best accuracy
abline(v = which(acc_k == max(acc_k)), col = "darkorange", lwd = 1.5)
# add line for max accuracy seen
abline(h = max(acc_k), col = "grey", lty = 2)
# First, install the keras R package from GitHub as follows:
devtools::install_github("rstudio/keras")
install.packages('recommenderlab'); install.packages('data.table')
n
n
y
adsf
library(recommenderlab)
library(data.table)
library(reshape2)
library(ggplot2)
install.packages('recommenderlab'); install.packages('data.table')
install.packages("recommenderlab")
install.packages("data.table")
install.packages("data.table")
install.packages('reshape2'); install.packages('ggplot2')
install.packages("reshape2")
install.packages("ggplot2")
install.packages("ggplot2")
1+1
library(recommenderlab)
library(data.table)
library(reshape2)
library(ggplot2)
library(recommenderlab)
library(data.table)
library(reshape2)
library(ggplot2)
# Read in the movies and ratings data sets
movies <- read.csv("data/movies.csv", stringsAsFactors = FALSE)
ratings <- read.csv("data/ratings.csv", stringsAsFactors = FALSE)
# Extract genre from movies data frame
genres <- as.data.frame(movies$genres, stringsAsFactors = FALSE)
genres <- as.data.frame(tstrsplit(genres[,1], '[|]', type.convert = TRUE), stringsAsFactors = FALSE)
colnames(genres) <- c(1:10)
genre_list <- c("Action", "Adventure", "Animation", "Children",
"Comedy", "Crime","Documentary", "Drama", "Fantasy",
"Film-Noir", "Horror", "Musical", "Mystery","Romance",
"Sci-Fi", "Thriller", "War", "Western")
# Creates a matrix for the number of movies + 1 and the number of genres
genre_matrix <- matrix(0, 9126, 18)
# Set first row to genre list
genre_matrix[1,] <- genre_list
# Set column names to genre list
colnames(genre_matrix) <- genre_list
# Iterate through matrix
for (i in 1:nrow(genres)) {
for (c in 1:ncol(genres)) {
genmat_col = which(genre_matrix[1,] == genres[i,c])
genre_matrix[i + 1,genmat_col] <- 1
}
}
# Convert the matrix into a data frame
# Remove first row, which was the genre list
genre_matrix <- as.data.frame(genre_matrix[-1,], stringsAsFactors = FALSE)
# Convert from characters to integers
for (c in 1:ncol(genre_matrix)) {
genre_matrix[,c] <- as.integer(genre_matrix[,c])
}
# Create search matrix
# Create ratings matrix. Rows = userId, Columns = movieId
# Remove userId's
# Convert rating matrix into a recommenderlab sparse matrix
# Selecting relevant data
# Select minimum number of users per rated movie
# and the minimum views per user
# Normalize the data
# Remove bias instances of data
# Averages the rating for each user to 0
# Define training and set data
# Building the recommendation model
# Applying the recommender system on the original dataset
# No. of recommendations we want per user
# Recommendations for the first user
# A matrix of recommendations for each user
# No. of recommendations we want per user
recommendations <- 10
